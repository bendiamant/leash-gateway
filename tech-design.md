# Leash Security â€” Envoyâ€‘based LLM Security Gateway (OSS)
## Tech Design v1.0

### ğŸ¯ Goal

An Envoyâ€‘based, OSS gateway that inspects and governs LLM requests through **configuration-based routing**, collects structured telemetry, and enforces pluggable policies. Applications route LLM traffic through the gateway by changing base URLs, enabling centralized governance without complex integrations.

**Deployment Models**: 
- **Self-hosted**: Enterprise deployment in customer infrastructure
- **SaaS**: Hosted multi-tenant service for faster adoption

**Key Insight**: Configuration-based routing (changing base URLs) provides the best balance of minimal integration effort and centralized control.

---

## ğŸ“‹ TL;DR

- **Integration**: Applications change base URL from `api.openai.com` to `gateway.company.com/v1/openai`
- **Data plane**: Envoy proxy with HTTP filters (ext_proc) for request interception and processing
- **Module runtime**: gRPC Module Host loads policy modules (Inspectors, Policies, Transformers, Sinks) dynamically
- **Provider routing**: Path-based routing (`/v1/openai`, `/v1/anthropic`) to different LLM providers
- **Deployment modes**: Self-hosted (customer infrastructure) or SaaS (hosted service)
- **Optional SDKs**: Enhanced features for teams wanting rich functionality
- **OSS**: Apacheâ€‘2.0, modular architecture, pluggable components

---

## ğŸ¯ Scope & Nonâ€‘Goals

### âœ… In Scope

- **Gateway Core**: Envoy-based proxy with configurable routing and policy enforcement
- **Configuration Integration**: Minimal changes (base URL update) for application integration
- **Policy Modules**: Pluggable modules for inspection, governance, transformation, and telemetry
- **Provider Support**: HTTP proxy to OpenAI, Anthropic, Google, AWS Bedrock, Cohere, etc.
- **Multi-deployment**: Same codebase for self-hosted and SaaS deployments
- **Optional SDKs**: Enhanced client libraries for teams wanting additional features
- **Observability**: Structured logging, metrics, and tracing for all LLM traffic

### âŒ Nonâ€‘Goals (v1.0)

- No application code rewriting requirements (beyond base URL configuration)
- No complex client-side integrations or mandatory SDK adoption
- No inâ€‘gateway model inference (external services for ML-based detection)
- No protocol translation beyond HTTP proxy functionality

---

## ğŸš€ Deployment Modes

### **Self-Hosted Deployment**
- Deploy in customer's infrastructure (VPC, on-premise, private cloud)
- Customer maintains full control of data and configuration
- Single-tenant deployment with customer-specific policies

### **SaaS Deployment**  
- Multi-tenant hosted service
- Tenant isolation through configuration and data separation
- Managed infrastructure and updates

### **Integration Method**
- **Configuration-based routing**: Applications change base URL configuration
- **Minimal changes**: `https://api.openai.com/v1` â†’ `https://gateway.company.com/v1/openai`
- **Works with any HTTP client**: No SDK requirement, existing code compatibility

---

## ğŸ—ï¸ Highâ€‘Level Architecture

### **Configuration-Based Routing Architecture**

```mermaid
graph TB
    subgraph "Applications (Minimal Config Change)"
        A["App A<br/>base_url: gateway.company.com/v1/openai"]
        B["App B<br/>base_url: gateway.company.com/v1/anthropic"]
        C["App C<br/>base_url: gateway.company.com/v1/google"]
    end
    
    subgraph "Leash Gateway"
        A --> E["Envoy Proxy<br/>(Path-based Routing)"]
        B --> E
        C --> E
        E --> |"ext_proc"| MH["Module Host<br/>(gRPC)"]
    end
    
    subgraph "Module Processing Pipeline"
        MH --> I["Inspectors<br/>(PII, Cost, Safety)"]
        I --> POL["Policies<br/>(Allow/Deny/Transform)"]
        POL --> T["Transformers<br/>(Redaction/Injection)"]
        T --> SINK["Sinks<br/>(Telemetry/Logging)"]
    end
    
    subgraph "LLM Providers"
        SINK --> P1["OpenAI<br/>api.openai.com"]
        SINK --> P2["Anthropic<br/>api.anthropic.com"]
        SINK --> P3["Google<br/>generativelanguage.googleapis.com"]
    end
    
    subgraph "Observability Stack"
        MH --> OBS["Metrics/Logs/Traces"]
        OBS --> PROM["Prometheus"]
        OBS --> OTEL["OpenTelemetry"]
        OBS --> DB["ClickHouse/Kafka"]
    end
```

### **Request Flow**

1. **Application**: Makes HTTP request to `gateway.company.com/v1/openai/chat/completions`
2. **Envoy**: Routes based on path (`/v1/openai` â†’ OpenAI provider)
3. **Module Host**: Processes request through policy pipeline
4. **Provider**: Forwards to actual LLM provider (`api.openai.com`)
5. **Response**: Returns through same pipeline with response processing

### **URL Routing Structure**

```
gateway.company.com/v1/openai/*     â†’ api.openai.com/v1/*
gateway.company.com/v1/anthropic/*  â†’ api.anthropic.com/v1/*
gateway.company.com/v1/google/*     â†’ generativelanguage.googleapis.com/v1/*
gateway.company.com/v1/bedrock/*    â†’ bedrock-runtime.{region}.amazonaws.com/*
```


---

## ğŸ“Š Data Model & Processing

### Request Processing Schema

```json
{
  "request_id": "req_2025091118351701",
  "timestamp": "2025-09-11T18:35:17Z",
  "tenant": "acme-corp",
  "source": {
    "service": "payments-api",
    "ip_address": "10.0.1.100",
    "user_agent": "python-requests/2.31.0",
    "headers": {
      "authorization": "Bearer sk-...[redacted]",
      "content-type": "application/json"
    }
  },
  "provider": {
    "name": "openai",
    "endpoint": "/v1/chat/completions",
    "detected_from": "url_path"
  },
  "llm_request": {
    "model": "gpt-4o-mini",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant"},
      {"role": "user", "content": "Summarize this document..."}
    ],
    "parameters": {
      "temperature": 0.2,
      "max_tokens": 1024,
      "stream": false
    }
  },
  "raw_request": {
    "method": "POST",
    "path": "/v1/openai/chat/completions",
    "size_bytes": 1247,
    "body_hash": "sha256:abc123..."
  }
}
```

### Response Processing Schema

```json
{
  "request_id": "req_2025091118351701",
  "timestamp": "2025-09-11T18:35:17Z",
  "tenant": "acme-corp",
  "provider": {
    "name": "openai",
    "model_requested": "gpt-4o-mini",
    "model_used": "gpt-4o-mini-2024-07-18",
    "endpoint_called": "https://api.openai.com/v1/chat/completions"
  },
  "llm_response": {
    "status_code": 200,
    "choices": [{
      "message": {"role": "assistant", "content": "Here's a summary..."},
      "finish_reason": "stop",
      "index": 0
    }],
    "system_fingerprint": "fp_abc123"
  },
  "usage": {
    "tokens": {"prompt": 150, "completion": 75, "total": 225},
    "cost_usd": 0.004500,
    "cost_calculation": {
      "prompt_tokens": 150,
      "completion_tokens": 75,
      "prompt_rate_per_1k": 0.010,
      "completion_rate_per_1k": 0.030
    }
  },
  "performance": {
    "gateway_overhead_ms": 3.2,
    "provider_latency_ms": 1243.8,
    "total_latency_ms": 1247.0,
    "module_processing_ms": 2.1
  },
  "policy_results": {
    "inspections": [
      {"module": "pii-detector", "detected": ["email"], "confidence": 0.95},
      {"module": "cost-tracker", "estimated_cost": 0.004500}
    ],
    "policy_decisions": [
      {"module": "content-filter", "action": "allow", "reason": "content_safe"}
    ],
    "transformations": [],
    "final_action": "allow"
  }
}
```

### Multi-Tenant Data Isolation

```yaml
# Tenant-specific configuration
tenant_config:
  tenant_id: "acme-corp"
  policies:
    - content-filter
    - cost-limiter
    - pii-detector
  quotas:
    requests_per_hour: 10000
    cost_limit_usd: 1000.00
  providers:
    openai:
      api_key_ref: "secret/acme-corp/openai"
      models: ["gpt-4o-mini", "gpt-4o"]
    anthropic:
      api_key_ref: "secret/acme-corp/anthropic"
      models: ["claude-3-sonnet-20240229"]
```

### ğŸ”’ Privacy Controls

- **Fieldâ€‘level redaction**: Configurable field masking
- **Hashing**: SHA-256 content fingerprinting  
- **TTL policies**: Automatic data expiration

---

## ğŸ”„ Request Flow

### ğŸ’» Application Integration

**Before (Direct Provider Call)**:
```python
# Application makes direct call to provider
client = OpenAI(
    api_key="sk-...",
    base_url="https://api.openai.com/v1"  # Direct to provider
)
```

**After (Gateway Routing)**:
```python
# Application routes through gateway (minimal change)
client = OpenAI(
    api_key="sk-...",
    base_url="https://gateway.company.com/v1/openai"  # Through gateway
)
```

### ğŸ  Gateway Processing Pipeline

1. **Request Interception**: Envoy receives HTTP request on path `/v1/{provider}/*`
2. **Provider Detection**: Extract provider from URL path (`/v1/openai` â†’ OpenAI)
3. **Module Processing**: Send to gRPC Module Host for policy evaluation
4. **Policy Pipeline**: Inspectors â†’ Policies â†’ Transformers â†’ Sinks
5. **Provider Forwarding**: Proxy request to actual provider endpoint
6. **Response Processing**: Apply response modules and return to application

### ğŸ” Module Processing Detail

```
HTTP Request â†’ Envoy â†’ ext_proc â†’ Module Host (gRPC)
                                        â”‚
                                        v
                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                   â”‚ Inspectors â”‚ â†’ Analyze content, detect PII, estimate cost
                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        v
                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                   â”‚  Policies   â”‚ â†’ Allow/deny/transform decisions
                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        v
                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                   â”‚Transformersâ”‚ â†’ Redact PII, inject context
                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        v
                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                   â”‚   Sinks    â”‚ â†’ Log to observability systems
                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        v
                              Forward to Provider
```

### ğŸŒŠ Streaming Support

1. **Request Processing**: Full request analyzed before streaming begins
2. **Stream Establishment**: Proxy streaming connection to provider
3. **Chunk Analysis**: Optional real-time processing of response chunks
4. **Stream Termination**: Ability to terminate stream on policy violations
5. **Response Logging**: Complete request/response logged after stream ends

### ğŸš« Error Handling

- **Module Failures**: Continue processing, log errors (fail-open for non-critical)
- **Policy Failures**: Block request (fail-closed for security)
- **Provider Failures**: Return error to application (no automatic fallbacks)
- **Gateway Failures**: Application falls back to direct provider calls

---

## âš™ï¸ Envoy Configuration (Reverse Proxy)

### HTTP Filters

```yaml
http_filters:
  # Rate limiting
  - name: envoy.filters.http.local_ratelimit
    typed_config:
      "@type": type.googleapis.com/envoy.extensions.filters.http.local_ratelimit.v3.LocalRateLimit
      stat_prefix: llm_gateway_rate_limit
      token_bucket:
        max_tokens: 1000
        tokens_per_fill: 100
        fill_interval: 1s
      filter_enabled:
        default_value:
          numerator: 100
          denominator: HUNDRED

  # External processing (module host)
  - name: envoy.filters.http.ext_proc
    typed_config:
      "@type": type.googleapis.com/envoy.extensions.filters.http.ext_proc.v3.ExternalProcessor
      grpc_service:
        envoy_grpc:
          cluster_name: module_host
        timeout: 2s
      processing_mode:
        request_body_mode: BUFFERED
        response_body_mode: STREAMED  # For streaming support
        request_header_mode: SEND
        response_header_mode: SEND
      message_timeout: 2s
      failure_mode_allow: false  # Fail closed
      max_message_timeout: 10s

  - name: envoy.filters.http.router
```

### Clusters

```yaml
clusters:
  - name: module_host
    connect_timeout: 1s
    type: STRICT_DNS
    load_assignment:
      cluster_name: module_host
      endpoints:
        - lb_endpoints:
            - endpoint:
                address:
                  socket_address:
                    address: module_host
                    port_value: 50051
    health_checks:
      - timeout: 1s
        interval: 5s
        grpc_health_check:
          service_name: "leash.ModuleHost"
    circuit_breakers:
      thresholds:
        - max_connections: 100
          max_requests: 1000
          max_retries: 3
```


---

## ğŸ§© Module System

### Module Types

| Type | Purpose | Examples | Input |
|------|---------|----------|-------|
| ğŸ” **Inspector** | Analyze HTTP request/response | PII detection, jailbreak detection, cost analysis | HTTP Request + Body |
| ğŸ›¡ï¸ **Policy** | Enforce governance rules | Allow/deny/warn decisions, quotas, rate limiting | HTTP Request + Inspector Results |
| ğŸ”§ **Transformer** | Modify request/response | Redact PII, inject headers, content filtering | HTTP Request/Response |
| ğŸ“¡ **Sink** | Export telemetry data | Structured logging, metrics, analytics | Complete Request/Response Context |

### Execution Phases

- **Request Processing**: `inspect_request` â†’ `evaluate_policies` â†’ `transform_request` â†’ `log_request`
- **Response Processing**: `inspect_response` â†’ `transform_response` â†’ `log_response`
- **Actions**: `CONTINUE`, `TRANSFORM`, `BLOCK`, `ANNOTATE`

### Module Implementation

- ğŸŸ¢ **Go Modules**: Native gRPC services, hot reload capability
- ğŸ¦€ **Plugin System**: Dynamic loading of compiled modules
- ğŸ”§ **Configuration**: YAML-based module configuration and routing

### gRPC Interface (Enhanced)

```protobuf
service Module {
  // Lifecycle management
  rpc Initialize(InitializeRequest) returns (InitializeResponse);
  rpc Healthcheck(HealthcheckRequest) returns (HealthcheckResponse);
  rpc Shutdown(ShutdownRequest) returns (ShutdownResponse);
  rpc GetMetadata(GetMetadataRequest) returns (ModuleMetadata);
  
  // Request processing
  rpc OnRequestHeaders(RequestHeadersContext) returns (HeaderAction);
  rpc OnRequestBody(RequestBodyContext) returns (BodyAction);
  rpc OnRequestComplete(RequestCompleteContext) returns (Action);
  
  // Response processing (streaming support)
  rpc OnResponseHeaders(ResponseHeadersContext) returns (HeaderAction);
  rpc OnResponseChunk(ResponseChunkContext) returns (ChunkAction);
  rpc OnResponseComplete(ResponseCompleteContext) returns (Action);
  
  // Configuration hot-reload
  rpc UpdateConfig(UpdateConfigRequest) returns (UpdateConfigResponse);
  rpc ValidateConfig(ValidateConfigRequest) returns (ValidateConfigResponse);
}
```

#### Enhanced Action Types

```protobuf
enum ActionKind {
  CONTINUE = 0;
  BLOCK = 1;
  TRANSFORM = 2;
  ANNOTATE = 3;
  RETRY = 4;     // New: Retry with backoff
  ROUTE = 5;     // New: Route to different provider
  CACHE = 6;     // New: Return cached response
}
```

#### Streaming Support

```protobuf
message ChunkAction {
  ActionKind kind = 1;
  bytes transformed_data = 2;
  bool terminate_stream = 3;  // Stop stream mid-response
  repeated Annotation annotations = 4;
}
```


---

## ğŸ”Œ Provider Routing & HTTP Proxy

### ğŸ—ºï¸ Path-Based Provider Routing

**URL Structure**:
```
gateway.company.com/v1/openai/*     â†’ api.openai.com/v1/*
gateway.company.com/v1/anthropic/*  â†’ api.anthropic.com/v1/*
gateway.company.com/v1/google/*     â†’ generativelanguage.googleapis.com/v1/*
gateway.company.com/v1/bedrock/*    â†’ bedrock-runtime.{region}.amazonaws.com/*
```

**Envoy Route Configuration**:
```yaml
route_config:
  name: llm_gateway_routes
  virtual_hosts:
  - name: llm_providers
    domains: ["*"]
    routes:
    - match:
        prefix: "/v1/openai/"
      route:
        cluster: openai_cluster
        prefix_rewrite: "/v1/"
    - match:
        prefix: "/v1/anthropic/"
      route:
        cluster: anthropic_cluster
        prefix_rewrite: "/v1/"
    - match:
        prefix: "/v1/google/"
      route:
        cluster: google_cluster
        prefix_rewrite: "/v1/"
```

### ğŸ”„ HTTP Proxy Implementation

**Provider Cluster Configuration**:
```yaml
clusters:
- name: openai_cluster
  type: LOGICAL_DNS
  load_assignment:
    cluster_name: openai_cluster
    endpoints:
    - lb_endpoints:
      - endpoint:
          address:
            socket_address:
              address: api.openai.com
              port_value: 443
  transport_socket:
    name: envoy.transport_sockets.tls
    typed_config:
      "@type": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext
      sni: api.openai.com
```

### ğŸ”§ Request/Response Processing

**Module Host gRPC Interface**:
```protobuf
service ModuleHost {
  rpc ProcessRequest(ProcessRequestRequest) returns (ProcessRequestResponse);
  rpc ProcessResponse(ProcessResponseRequest) returns (ProcessResponseResponse);
}

message ProcessRequestRequest {
  string request_id = 1;
  string tenant_id = 2;
  string provider = 3;  // Extracted from URL path
  HttpRequest http_request = 4;
  map<string, string> headers = 5;
  bytes body = 6;
}

message ProcessRequestResponse {
  Action action = 1;  // CONTINUE, BLOCK, TRANSFORM
  bytes modified_body = 2;  // If transformed
  map<string, string> additional_headers = 3;
  string block_reason = 4;  // If blocked
}
```

### ğŸ” Optional SDK Enhancement

**For teams wanting rich features**:
```python
# Optional: Enhanced SDK with additional features
from leash_ai import LeashLLM

# SDK can provide fallbacks, caching, etc.
leash = LeashLLM(
    gateway_url="https://gateway.company.com",
    fallback_providers=["openai", "anthropic"],
    cache_enabled=True,
    retry_attempts=3
)

# Same OpenAI interface, enhanced reliability
response = leash.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "Hello"}]
)
```

**SDK Features** (Optional):
- Automatic fallbacks between providers
- Client-side caching
- Retry logic with exponential backoff
- Request/response transformation
- Framework integrations (LangChain, etc.)

**Core Principle**: Gateway works with any HTTP client, SDK provides enhancements

---

## ğŸ” Identity & Key Management

- **Inbound Authentication**: mTLS/JWT validation
- **Outbound Keys**: Provider keys from KMS, injected by policy
- **Key Stripping**: Strip inbound keys, re-inject approved credentials

---

## ğŸ“Š Observability

### Metrics
- **Request metrics**: Rate, latency, success/error rates
- **Business metrics**: Token counts, costs, safety scores
- **Module metrics**: Processing time, failure rates

### Tracing
- **OpenTelemetry spans**: End-to-end request tracing
- **Module chain visibility**: Per-module execution tracking

### Events & Sinks
- **ClickHouse**: Analytics and reporting
- **Kafka**: Real-time event streaming
- **OTLP**: OpenTelemetry Protocol export

---

## ğŸ”’ Privacy & Compliance

### Data Protection
- **Configurable redaction**: Field-level masking before persistence
- **TTL policies**: Per-field data expiration
- **Encryption**: At-rest and in-transit

### Access Control
- **Tenant-scoped RBAC**: Multi-tenant isolation
- **Data residency tags**: Geographic data placement
- **Audit trails**: Complete request/response logging

â¸»

## âš¡ Performance & SLI/SLO

### ğŸ¯ Service Level Objectives

| Metric | Target | Measurement |
|--------|--------|--------------|
| Gateway Latency P50 | â‰¤4ms | Overhead only |
| Gateway Latency P95 | â‰¤10ms | Overhead only |
| Module Processing P95 | â‰¤2ms | Per module |
| Request Success Rate | â‰¥99.9% | Non-policy blocks |
| Module Availability | â‰¥99.95% | Health check based |
| Provider Success Rate | â‰¥99.5% | Excluding rate limits |

### ğŸ“Š Monitoring Stack

- **Metrics**: Prometheus with custom business metrics (cost, safety, policy decisions)
- **Tracing**: OpenTelemetry spans across module chain
- **Alerting**: Error budget burn rate alerts (fast: 14.4x, slow: 3x)
- **Dashboards**: Grafana with SLO compliance tracking

### ğŸ’ª Load Testing

- **Baseline**: 1000 RPS steady state
- **Peak**: 5000 RPS burst capacity
- **Streaming**: 10% of traffic, latency within SLO
- **Module Stress**: Heavy processing (PII, content filter, cost tracking)

### ğŸ“Š Performance Limits

- **Request Body**: 10MB max
- **Module Memory**: Configurable per module (default 256MB)
- **Module CPU**: Max 50% of container
- **Streaming**: Backpressure handling for slow modules

â¸»

## ğŸ›¡ï¸ Security Considerations

### ğŸš« Fail-Safe Defaults
- **Fail-closed**: All policy failures block requests
- **Default deny**: Explicit allow rules required

### ğŸ›¡ï¸ DoS Protection
- **Request limits**: 10MB max body size
- **Rate limiting**: 1000 RPS default
- **Circuit breakers**: Per-provider failure thresholds

### ğŸ“¼ Module Sandboxing
- **Resource limits**: Memory, CPU, timeout enforcement
- **Crash isolation**: Module failures don't affect gateway
- **Signature verification**: Signed module binaries

### ğŸ” Key Management
- **Provider keys**: Sourced from KMS
- **Key stripping**: Remove inbound credentials
- **Least privilege**: Minimal key injection

### ğŸ“ Audit & Privacy
- **Complete audit trail**: Request/response logging with correlation IDs
- **Field-level redaction**: Configurable PII masking
- **Tenant isolation**: Multi-tenant data separation

### ğŸ”’ Supply Chain Security
- **SBOM generation**: Software bill of materials
- **Signed releases**: Cryptographic signatures
- **Vulnerability scanning**: Automated security checks

### ğŸŒ Network Security
- **mTLS**: Inter-component encryption
- **Encryption at rest**: Data storage protection

### ğŸ“„ Compliance
- **SOC2**: Security controls framework
- **GDPR**: Data protection compliance
- **Audit log retention**: Configurable retention policies

---

## ğŸ› ï¸ SDK Architecture & Framework Integration

### Multi-Language SDK Support

#### Python SDK Features
```python
from leash_ai import LeashLLM
from leash_ai.integrations import LangChainAdapter, CrewAIAdapter

# Direct usage
leash = LeashLLM(gateway_url="https://gateway.company.com")

# Framework integration
langchain_llm = LangChainAdapter(leash)
crewai_llm = CrewAIAdapter(leash)

# Configuration-driven provider selection
config = {
    "strategy": {"mode": "fallback"},
    "targets": [
        {"provider": "openai", "model": "gpt-4o-mini"},
        {"provider": "anthropic", "model": "claude-3-sonnet-20240229"}
    ]
}

response = leash.chat.completions.create(
    messages=[{"role": "user", "content": "Hello"}],
    config=config
)
```

#### TypeScript SDK Features
```typescript
import { LeashLLM } from '@leash-security/sdk'
import { createOpenAI } from 'ai'  // Vercel AI SDK integration

// Direct usage
const leash = new LeashLLM({
  gatewayUrl: 'https://gateway.company.com',
  apiKey: process.env.LEASH_API_KEY
})

// Vercel AI SDK integration
const openai = createOpenAI({
  baseURL: leash.baseURL,
  apiKey: leash.apiKey,
  defaultHeaders: leash.defaultHeaders
})

// Load balancing configuration
const response = await leash.chat.completions.create({
  model: 'gpt-4o-mini',
  messages: [{ role: 'user', content: 'Hello' }]
}, {
  config: {
    strategy: { mode: 'loadbalance' },
    targets: [
      { provider: 'openai', weight: 0.7 },
      { provider: 'anthropic', weight: 0.3 }
    ]
  }
})
```

### Agent Framework Integrations

| Framework | SDK Support | Features | Status |
|-----------|-------------|----------|---------|
| **LangChain** | Python, TypeScript | Chain integration, memory, tools | âœ… |
| **CrewAI** | Python | Agent orchestration, role-based | âœ… |
| **AutoGen** | Python | Multi-agent conversations | ğŸŸ¡ |
| **Vercel AI SDK** | TypeScript | Streaming, UI components | âœ… |
| **LlamaIndex** | Python | RAG, document processing | ğŸŸ¡ |

### SDK Configuration Management

```python
# Environment-based configuration
leash = LeashLLM.from_env()  # Uses LEASH_GATEWAY_URL, LEASH_API_KEY

# Configuration file
leash = LeashLLM.from_config("leash-config.yaml")

# Dynamic configuration
leash.update_config({
    "default_provider": "anthropic",
    "fallback_provider": "openai",
    "retry_attempts": 3,
    "timeout_seconds": 30
})
```

---

## ğŸ“š OSS Repository Layout

```
leash-security/
â”œâ”€â”€ gateway/                    # Core gateway
â”‚   â”œâ”€â”€ envoy/                 # Envoy bootstrap configs
â”‚   â”œâ”€â”€ module-host/           # gRPC module runtime
â”‚   â”œâ”€â”€ provider-adapters/     # Pluggable provider adapters
â”‚   â”‚   â”œâ”€â”€ openai/           # OpenAI adapter
â”‚   â”‚   â”œâ”€â”€ anthropic/        # Anthropic adapter
â”‚   â”‚   â”œâ”€â”€ google/           # Google Gemini adapter
â”‚   â”‚   â””â”€â”€ community/        # Community-contributed adapters
â”‚   â””â”€â”€ modules/              # Built-in policy modules
â”‚       â”œâ”€â”€ inspectors/       # PII, cost, safety analysis
â”‚       â”œâ”€â”€ policies/         # Allow/deny/quota rules
â”‚       â”œâ”€â”€ transformers/     # Content modification
â”‚       â””â”€â”€ sinks/            # Observability exports
â”‚
â”œâ”€â”€ sdks/                      # Multi-language SDKs
â”‚   â”œâ”€â”€ python/               # Python SDK with framework integrations
â”‚   â”‚   â”œâ”€â”€ leash_ai/         # Core SDK
â”‚   â”‚   â”œâ”€â”€ integrations/     # LangChain, CrewAI, AutoGen
â”‚   â”‚   â””â”€â”€ examples/         # Usage examples
â”‚   â”œâ”€â”€ typescript/           # TypeScript/JavaScript SDK
â”‚   â”‚   â”œâ”€â”€ packages/         # Core SDK + framework adapters
â”‚   â”‚   â”œâ”€â”€ examples/         # React, Next.js examples
â”‚   â”‚   â””â”€â”€ integrations/     # Vercel AI SDK, LangChain.js
â”‚   â””â”€â”€ go/                   # Go SDK (native)
â”‚
â”œâ”€â”€ examples/                  # Deployment & usage examples
â”‚   â”œâ”€â”€ docker-compose/       # Local development setup
â”‚   â”œâ”€â”€ kubernetes/           # K8s manifests
â”‚   â”œâ”€â”€ terraform/            # Infrastructure as code
â”‚   â”œâ”€â”€ agent-frameworks/     # LangChain, CrewAI examples
â”‚   â””â”€â”€ use-cases/            # Real-world scenarios
â”‚
â”œâ”€â”€ docs/                      # Documentation
â”‚   â”œâ”€â”€ gateway/              # Gateway configuration
â”‚   â”œâ”€â”€ sdks/                 # SDK documentation
â”‚   â”œâ”€â”€ modules/              # Module development
â”‚   â””â”€â”€ adapters/             # Provider adapter development
â”‚
â”œâ”€â”€ tools/                     # Development tools
â”‚   â”œâ”€â”€ cli/                  # Leash CLI tool
â”‚   â”œâ”€â”€ testing/              # Testing utilities
â”‚   â””â”€â”€ benchmarks/           # Performance benchmarks
â”‚
â”œâ”€â”€ LICENSE                    # Apache 2.0
â”œâ”€â”€ SECURITY.md               # Security policy
â””â”€â”€ CONTRIBUTING.md           # Contribution guidelines
```


---

## ğŸ”„ Module Lifecycle Management

### Module State Machine

```mermaid
stateDiagram-v2
    [*] --> LOADING
    LOADING --> INITIALIZING
    LOADING --> FAILED
    INITIALIZING --> READY
    INITIALIZING --> FAILED
    READY --> RUNNING
    RUNNING --> DRAINING
    RUNNING --> FAILED
    DRAINING --> STOPPED
    STOPPED --> [*]
    FAILED --> [*]
```

### ğŸ”„ Hot Reload Strategy

- **Blue-green deployment**: Zero-downtime updates
- **Health checks**: 5s interval, 3 failure threshold
- **Automatic rollback**: On health check failures
- **Configuration validation**: Schema + dependency checks

### ğŸ”— Module Chain Orchestration

| Phase | Execution | Failure Mode |
|-------|-----------|-------------|
| ğŸ” **Inspectors** | Parallel | Skip and continue |
| ğŸ›¡ï¸ **Policies** | Sequential | Fail-fast (BLOCK) |
| ğŸ”§ **Transformers** | Sequential | Rollback on failure |
| ğŸ“¡ **Sinks** | Fire-and-forget | Retry with queues |

### ğŸš« Error Recovery

- **Module crashes**: Exponential backoff restart
- **Config errors**: Rollback to last known good
- **Partial failures**: Degraded mode with warnings
- **Resource exhaustion**: Throttling and graceful degradation

---

## âš™ï¸ Configuration Management

### Configuration Hierarchy

```yaml
# Global settings
global:
  cluster_id: "prod-us-east-1"
  log_level: "info"

# Envoy configuration
envoy:
  bootstrap: "envoy.yaml"
  admin_port: 9901

# Module host settings
module_host:
  grpc_port: 50051
  health_port: 8080
  module_dir: "/etc/leash/modules"

# Per-module configuration
modules:
  content-filter:
    enabled: true
    config:
      threshold: 0.8
      action: "block"
```

### ğŸ”„ Hot Configuration Reload

1. **File Watcher**: fsnotify-based change detection
2. **Validation Pipeline**: Schema â†’ Dependencies â†’ Dry-run
3. **Atomic Updates**: Blue-green config deployment
4. **Rollback**: Automatic on validation failure

### ğŸš€ Deployment Strategies

- **Canary**: 10% traffic split with success criteria
- **Blue-Green**: Zero-downtime updates with health verification
- **A/B Testing**: Traffic splitting for production validation
- **GitOps**: Version-controlled config with approval workflows

---

## ğŸš« Error Handling & Resilience

### Error Categories

| Category | Examples | Strategy |
|----------|----------|----------|
| **Module Chain** | Timeout, crash, invalid response | Skip non-critical, fail-closed for policies |
| **Provider** | 5xx errors, timeouts | Circuit breaker + exponential backoff |
| **Configuration** | Validation failure | Hot-reload rollback |
| **Resources** | Memory/CPU limits | Throttling + graceful degradation |

### ğŸ”„ Circuit Breaker Configuration

```yaml
circuit_breaker:
  failure_threshold: 50%    # Error rate threshold
  min_requests: 10          # Minimum requests before evaluation
  timeout: 30s              # Recovery attempt interval
  half_open_requests: 1     # Test requests in half-open state
```

### ğŸŒŠ Streaming Error Handling

- **Mid-stream blocking**: Graceful termination on policy violation
- **Partial responses**: Continue with warnings for non-critical errors
- **Backpressure**: Timeout handling for slow modules
- **Recovery**: Automatic retry for transient failures

### ğŸ“Š Failure Mode Matrix

| Module Type | Timeout | Crash | Invalid Response |
|-------------|---------|-------|-----------------|
| Inspector | Skip | Skip | Log & Continue |
| Policy | **Fail Closed** | **Fail Closed** | **Fail Closed** |
| Transformer | Skip | Skip | Use Original |
| Sink | Queue | Queue | Log & Continue |

---

## ğŸ›£ï¸ Roadmap

### ğŸ Phase 1 (v1.0) - Core Gateway + SDKs

- âœ… **Client SDKs**: Python, TypeScript, Go with unified interface
- âœ… **Gateway Core**: Envoy + ext_proc for normalized request processing
- âœ… **Provider Adapters**: OpenAI, Anthropic adapters (pluggable)
- âœ… **Module System**: Inspector, Policy, Transformer, Sink modules
- âœ… **Observability**: Prometheus metrics, OpenTelemetry tracing
- âœ… **Essential Policies**: Rate limiting, content filtering, cost tracking

### ğŸ›¡ï¸ Phase 1.5 (v1.0.1) - Production Hardening

- ğŸŸ¡ **Streaming Support**: SSE/chunked responses through adapters
- ğŸŸ¡ **SDK Resilience**: Fallback logic, retry policies, circuit breakers
- ğŸŸ¡ **Configuration Hot-reload**: Dynamic adapter and module updates
- ğŸŸ¡ **Security Hardening**: DoS protection, audit trails, policy validation
- ğŸŸ¡ **Performance**: â‰¤4ms gateway overhead, SDK-level caching
- ğŸŸ¡ **Framework Integration**: LangChain, AutoGen, CrewAI support

### ğŸŒ Phase 2 (v1.1) - Extended Ecosystem

- âšª **Provider Expansion**: Google Gemini, AWS Bedrock, Cohere adapters
- âšª **Community Adapters**: Plugin marketplace for custom providers
- âšª **Advanced Analytics**: ClickHouse sink, cost optimization insights
- âšª **Multi-modal Support**: Image, audio processing through normalized schema
- âšª **Forward Proxy Mode**: MITM capabilities for legacy applications

### ğŸ¤– Phase 3 (v1.2) - Advanced Intelligence

- âšª **Advanced Modules**: ML-based anomaly detection, semantic analysis
- âšª **Smart Routing**: Cost-aware, latency-optimized provider selection
- âšª **Policy Engine**: CEL expressions, complex rule evaluation
- âšª **Sidecar Mode**: Service mesh integration, per-service policies
- âšª **SDK Intelligence**: Auto-fallback, predictive caching

### ğŸ¢ Phase 4 (v1.3) - Enterprise Platform

- âšª **Unified API**: Optional abstraction layer over all providers
- âšª **Enterprise Analytics**: Advanced reporting, cost allocation, usage insights
- âšª **Compliance Suite**: SOC2, GDPR, HIPAA automated compliance
- âšª **Multi-tenancy**: Complete tenant isolation, RBAC, audit trails
- âšª **Enterprise Integration**: SSO, LDAP, enterprise monitoring systems

---

## ğŸ“‹ Example Policy (CEL)

```javascript
// Deny high temperature in production
request.tenant == 'prod' && request.parameters.temperature > 0.7
  ? deny("High temperature not allowed", reason="policy.temp")
  : allow()
```

```javascript
// Block PII in prompts
has(request.annotations.pii_detected) && size(request.annotations.pii_detected) > 0
  ? deny("PII detected in prompt", reason="policy.pii")
  : allow()
```

```javascript
// Cost-based throttling
request.estimated_cost > 1.0 && request.tenant != 'enterprise'
  ? deny("Cost limit exceeded", reason="policy.cost")
  : allow()
```


---

## ğŸ¹ Example Go Module

```go
package main

import (
    "context"
    "fmt"
    
    pb "github.com/leash-security/gateway/proto"
)

type TemperaturePolicy struct {
    maxTemp float64
}

func (m *TemperaturePolicy) OnRequest(ctx context.Context, req *pb.NormalizedRequest) (*pb.Action, error) {
    // Check temperature parameter
    if temp, exists := req.Parameters["temperature"]; exists {
        if temp.GetNumberValue() > m.maxTemp {
            return &pb.Action{
                Kind:       pb.Action_BLOCK,
                HttpStatus: 400,
                Message:    fmt.Sprintf("Temperature %.2f exceeds limit %.2f", temp.GetNumberValue(), m.maxTemp),
                Reason:     "policy.temperature",
                Annotations: []*pb.Annotation{{
                    Key:   "violation_type",
                    Value: &pb.Annotation_StringValue{StringValue: "temperature_limit"},
                    Type:  pb.AnnotationType_POLICY,
                }},
            }, nil
        }
    }
    
    return &pb.Action{Kind: pb.Action_CONTINUE}, nil
}

func (m *TemperaturePolicy) Initialize(ctx context.Context, req *pb.InitializeRequest) (*pb.InitializeResponse, error) {
    m.maxTemp = 0.7 // Default temperature limit
    return &pb.InitializeResponse{Success: true}, nil
}

func (m *TemperaturePolicy) Healthcheck(ctx context.Context, req *pb.HealthcheckRequest) (*pb.HealthcheckResponse, error) {
    return &pb.HealthcheckResponse{
        Status:  pb.HealthStatus_HEALTHY,
        Message: "Temperature policy module is healthy",
    }, nil
}
```

---

## ğŸš€ Getting Started

### Gateway Deployment

```bash
# Self-hosted deployment
docker run -p 8080:8080 \
  -e OPENAI_API_KEY=sk-... \
  -e ANTHROPIC_API_KEY=ant-... \
  leash-security/gateway

# Kubernetes deployment
kubectl apply -f https://raw.githubusercontent.com/leash-security/gateway/main/examples/k8s/
```

### Application Integration

#### Minimal Configuration Change
```python
# Before: Direct provider call
client = OpenAI(
    api_key="sk-...",
    base_url="https://api.openai.com/v1"
)

# After: Route through gateway (one line change)
client = OpenAI(
    api_key="sk-...",
    base_url="https://gateway.company.com/v1/openai"  # Only change needed
)

# Application code remains identical
response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "Hello world"}]
)
```

#### Environment Variable Configuration
```bash
# Set base URL via environment variable
export OPENAI_BASE_URL="https://gateway.company.com/v1/openai"
export ANTHROPIC_BASE_URL="https://gateway.company.com/v1/anthropic"

# Application uses environment variables (no code changes)
python my_app.py
```

### Multi-Provider Example
```python
# Different providers, same gateway
openai_client = OpenAI(base_url="https://gateway.company.com/v1/openai")
anthropic_client = Anthropic(base_url="https://gateway.company.com/v1/anthropic")

# All traffic flows through same gateway for governance
openai_response = openai_client.chat.completions.create(...)
anthropic_response = anthropic_client.messages.create(...)
```

### Module Development

```bash
# Generate module template
leash module create --name content-filter --type policy

# Build and deploy
go build -o content-filter.so -buildmode=plugin ./content-filter
leash module deploy content-filter.so
```

### Configuration Management

```yaml
# gateway-config.yaml
modules:
  - name: pii-detector
    type: inspector
    enabled: true
  - name: cost-limiter
    type: policy
    enabled: true
    config:
      daily_limit_usd: 1000.00

providers:
  openai:
    endpoint: "https://api.openai.com/v1"
    models: ["gpt-4o-mini", "gpt-4o"]
  anthropic:
    endpoint: "https://api.anthropic.com/v1"
    models: ["claude-3-sonnet-20240229"]
```

---

*Built with â¤ï¸ by the Leash Security team. Licensed under Apache 2.0.*